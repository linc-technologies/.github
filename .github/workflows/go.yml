name: "Go"

on:
  workflow_call:

jobs:
  build:
    name: "Build"
    runs-on: ubuntu-latest
    outputs:
      build_number: ${{ steps.export_build_tag.outputs.build_number }}
      docker_build_number: ${{ steps.export_build_tag.outputs.docker_build_number }}
    steps:

      - name: "Checkout"
        id: checkout
        uses: actions/checkout@v3

      - name: "Git HTTPS"
        id: git_https
        uses: linc-technologies/.github/.github/actions/git_http@master
        with:
          pat: ${{ secrets.CICD_TOKEN }}

      - name: "Setup Go"
        id: setup_go
        uses: actions/setup-go@v4
        with:
          go-version: '1.19' # The Go version to download (if necessary) and use.
#          cache: true $ TODO: Throwing issues with repeat builds....

      - name: "Format"
        if: ${{ github.ref_name != 'production' }}
        id: format
        run: if [ "$(gofmt -l . | wc -l)" -gt 0 ]; then exit 1; fi

      - name: "Lint"
        id: lint
        uses: golangci/golangci-lint-action@v3
        env:
          GOPRIVATE: github.com/linc-technologies
        with:
          # Required: the version of golangci-lint is required and must be specified without patch version: we always use the latest patch version.
          version: v1.52
          # https://golangci-lint.run/usage/configuration/#command-line-options
          args: --issues-exit-code=0 --build-tags=internal --go=1.19 --timeout=2m --verbose

      - name: "Build"
        id: build
        env:
          GOPRIVATE: github.com/linc-technologies
          CGO_ENABLED: 0
        run: |
          cd ./cmd/${{ github.event.repository.name }}/
          go build -tags internal -ldflags="-s -w" -v

      - name: "Setup Mongo"
        id: setup_mongo
        if: github.event.repository.name != 'finance'
        uses: linc-technologies/.github/.github/actions/service_mongo@master

      - name: "Setup MySQL"
        id: setup_mysql
        if: github.event.repository.name == 'finance'
        uses: linc-technologies/.github/.github/actions/service_mysql@master

      - name: "Test"
        id: test
        run: go test -vet=off -tags internal -v ./...

      - name: "Set up Docker Buildx"
        id: set_up_docker_buildx
        uses: docker/setup-buildx-action@v2

      # TODO: Create a new workflow for dependabot with only composite actions to limit steps like these
      - name: "Login to ACR"
        id: login_to_acr
        if: ${{ github.actor != 'dependabot[bot]' }}
        uses: docker/login-action@v2
        with:
          registry: linced.azurecr.io
          username: ${{ secrets.AZURE_ACR_LINCED_USERNAME }}
          password: ${{ secrets.AZURE_ACR_LINCED_PASSWORD }}

      - name: "Export Build Tag"
        id: export_build_tag
        uses: linc-technologies/.github/.github/actions/build_label@master

      - name: "Check Dockerfile.release exists"
        id: check_files
        uses: andstor/file-existence-action@v2
        with:
          files: "Dockerfile.release"

      - name: "Build and export to Docker"
        id: build_and_export_to_docker
        uses: docker/build-push-action@v4
        if: ${{ !contains(fromJson('["development", "staging", "production"]'), github.ref_name) }}
        with:
          context: .
          push: false
          tags: ${{ steps.export_build_tag.outputs.docker_build_number }}
          file: ${{ steps.check_files.outputs.files_exists == 'true' && "Dockerfile.release" || "Dockerfile.drone" }}

      - name: "Build and export to Docker (Environment Based)"
        id: build_and_export_to_docker_environment_based
        uses: docker/build-push-action@v4
        if: contains(fromJson('["development", "staging", "production"]'), github.ref_name)
        with:
          context: .
          push: true
          tags: |
            ${{ steps.export_build_tag.outputs.docker_build_number }}
            ${{ steps.export_build_tag.outputs.docker_build_branch }}
          file: ${{ steps.check_files.outputs.files_exists == 'true' && "Dockerfile.release" || "Dockerfile.drone" }}

      - name: "Slack Notify"
        if: success() || failure()
        uses: linc-technologies/.github/.github/actions/slack_notify@master
        continue-on-error: true
        with:
          event: build
          success: ${{ !contains(steps.*.conclusion, 'failure') }}
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

  deploy:
    name: "Deploy"
    if: contains(fromJson('["testing", "development", "staging", "production"]'), github.ref_name)
    environment: ${{ github.ref_name }}
    runs-on: ubuntu-latest
    needs: [build]
    outputs:
      pre_deploy_revision: ${{ steps.export.outputs.pre_deploy_revision }}
      post_deploy_revision: ${{ steps.export.outputs.post_deploy_revision }}
      deploy_name: ${{ steps.export.outputs.deploy_name }}
      deploy_namespace: ${{ steps.export.outputs.deploy_namespace }}
    steps:

      - name: "Checkout"
        id: checkout
        uses: actions/checkout@v3
        with:
          repository: linc-technologies/helm
          token: ${{ secrets.CICD_TOKEN }}

      - name: "Install Kubectl"
        id: install_kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.25.5'

      # If we're deploying from a non-production branch, configure Kubectl with Dev cluster secrets
      - name: "Configure Kubectl (development-au-east)"
        id: configure_kubectl_development_au_east
        if: ${{ github.ref != 'refs/heads/production' }}
        uses: linc-technologies/.github/.github/actions/k8s_auth@master
        with:
          server: ${{ secrets.DEVELOPMENT_AU_EAST_AKS_SERVER }}
          token: ${{ secrets.DEVELOPMENT_AU_EAST_AKS_TOKEN }}
          cert: ${{ secrets.DEVELOPMENT_AU_EAST_AKS_CERT }}

      # If we're deploying from a production branch, configure Kubectl with Prod cluster secrets
      - name: "Configure Kubectl (production-au-east)"
        id: configure_kubectl_production_au_east
        if: ${{ github.ref == 'refs/heads/production' }}
        uses: linc-technologies/.github/.github/actions/k8s_auth@master
        with:
          server: ${{ secrets.PRODUCTION_AU_EAST_AKS_SERVER }}
          token: ${{ secrets.PRODUCTION_AU_EAST_AKS_TOKEN }}
          cert: ${{ secrets.PRODUCTION_AU_EAST_AKS_CERT }}

      - name: "Kubectl Version"
        id: kubectl_version
        run: kubectl version

      # Used by the post-deploy steps
      - name: "Set Outputs"
        id: export
        env:
          CONTAINER: ${{ github.event.repository.name }}
          DEPLOYMENT: ${{ github.event.repository.name }}
          NAMESPACE: ${{ github.ref_name }}
          IMAGE: ${{ needs.build.outputs.docker_build_number }}
        run: |
          PRE_DEPLOY_REVISION=$(kubectl get deployment -n $NAMESPACE $DEPLOYMENT -o=jsonpath={.spec.template.spec.containers[0].image})  
          POST_DEPLOY_REVISION=$IMAGE  
          DEPLOY_NAME=$DEPLOYMENT  
          DEPLOY_NAMESPACE=$NAMESPACE 
          
          echo "pre_deploy_revision=${PRE_DEPLOY_REVISION}" >> $GITHUB_OUTPUT
          echo "post_deploy_revision=${POST_DEPLOY_REVISION}" >> $GITHUB_OUTPUT
          echo "deploy_name=${DEPLOY_NAME}" >> $GITHUB_OUTPUT
          echo "deploy_namespace=${DEPLOY_NAMESPACE}" >> $GITHUB_OUTPUT

      - name: "Helm Deploy"
        id: helm_deploy
        uses: linc-technologies/.github/.github/actions/deploy_helm@master
        with:
          chart: "service"
          release: ${{ github.event.repository.name }}
          namespace: ${{ github.ref_name }}
          tag: ${{ needs.build.outputs.build_number }}
          oci_user: ${{ secrets.AZURE_ACR_LINCED_USERNAME }}
          oci_pass: ${{ secrets.AZURE_ACR_LINCED_PASSWORD }}

      - name: "Slack Notify"
        if: success() || failure()
        uses: linc-technologies/.github/.github/actions/slack_notify@master
        continue-on-error: true
        with:
          event: deploy
          success: ${{ !contains(steps.*.conclusion, 'failure') }}
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

  post-deploy:
    name: "Post Deploy"
    if: ${{ github.ref_name == 'production' }}
    permissions:
      pull-requests: write
    runs-on: ubuntu-latest
    needs: [deploy]
    continue-on-error: true
    steps:

      - name: "Find PR"
        id: pr
        uses: 8BitJonny/gh-get-current-pr@2.2.0

      # Debug PR if found
      - run: echo "PR ${prNumber} ${prTitle} at ${prUrl}"
        if: steps.pr.outputs.pr_found == 'true'
        env:
          prNumber: ${{ steps.pr.outputs.number }}
          prTitle: ${{ steps.pr.outputs.pr_title }}
          prUrl: ${{ steps.pr.outputs.pr_url }}

      - name: "Update PR with Rollback Steps"
        if: steps.pr.outputs.pr_found == 'true'
        uses: actions/github-script@v6
        env:
          ROLLBACK_VERSION: ${{ needs.deploy.outputs.pre_deploy_revision }}
          DEPLOYMENT_VERSION: ${{ needs.deploy.outputs.post_deploy_revision }}
          DEPLOYMENT_NAME: ${{ needs.deploy.outputs.deploy_name }}
          NAMESPACE: ${{ needs.deploy.outputs.deploy_namespace }}
          PR: ${{ steps.pr.outputs.number }}
        with:
          script: |
            const { ROLLBACK_VERSION, DEPLOYMENT_VERSION, DEPLOYMENT_NAME, NAMESPACE, PR } = process.env

            github.rest.issues.createComment({
              issue_number: PR,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `ðŸš€ Whoosh! âœ¨ There's your code, off she goes! ðŸ§™

            **${ROLLBACK_VERSION} => ${DEPLOYMENT_VERSION}** 

            You can view the history and status of the release with:
            \`\`\`console
            kubectl config use-context production-au-east-aks
            helm -n ${NAMESPACE} history ${DEPLOYMENT_NAME}
            helm -n ${NAMESPACE} status ${DEPLOYMENT_NAME}
            \`\`\`

            Or, to rollback:
            \`\`\`console
            # Use the correct context
            kubectl config use-context production-au-east-aks
            
            # Perform a diff on the rollback to ensure everything is cromulent
            helm -n ${NAMESPACE} diff rollback ${DEPLOYMENT_NAME} <version>
            
            # Execute the final rollback
            helm -n ${NAMESPACE} rollback ${DEPLOYMENT_NAME}
            \`\`\`
            `})
